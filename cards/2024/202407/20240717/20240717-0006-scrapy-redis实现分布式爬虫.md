# 0006-scrapy-redis实现分布式爬虫

## 标签

## 内容

scrapy 做大规模的分布式应用时，有点困难
通过改变 scrapy的队列调度，将起始的网址从 start_urls中分离，改从 redis 读取。
这样多个客户端可以同时读取同一个redis,从而实现了分布式爬虫，通过在 settings.py 中设定的redis相同客户端连接信息的话，就可以实现读取同一个redis

优点：

1. 断点续爬，这次抓取的URL，下次就不会再次爬取
2. 分布式快速爬取，多个电脑一起爬取数据，不会重复URL，并且多个电脑的性能强大

> 所有待抓取的request对象和去重的request对象指纹（md5方式，或者其他）保存在共用的redis中
> 所有的scrapy连接相同的redis，也就共用了request对象的队列
> 所有scray创建的request对象，在保存到redis的request对象队列前，会先通过指纹判断是否重复，重复的话过滤，不重复的添加到队列中
> 默认情况下所有数据会保存到redis中

## 参考
